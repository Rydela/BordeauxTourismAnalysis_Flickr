---
title: "Analyzing Evolution of Tourist Movement in Bordeaux"
author: "Ryan Arias Delafosse"
date: "2/12/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data Analysis

The purpose of this project is to analyze and visualize how tourist movement changes over times in the City of Bordeaux. By extracting insights from social media tagged data, we are able to learn more about the "paths" tourist take within the city. By creating these paths in different windows of time we can research whether these paths change between these windows of time.

### Data Extraction and Preparation

Data was taken from Flickr from the years 2005 to 2018. The photos chosens are ones tagged within the area of Bordeax.

The data was saved by year, then binded into one singular dataset. With one dataset, we can be sure that the same clearning techniques are applied equally.

### Load Libraries

```{r load libraries}
# Load library
library(dplyr)
library(readr)
library(ggplot2)
library(dbscan)
library(leaflet)
```

### Import Dataset

We import the singular dataset of all years combined, dilleanted by a tab. We make sure the latitude and longitude columns are declared numeric.

```{r Import Merged Dataset, echo=FALSE}

# Get a List of all files in directory named with a keyword, say all `.csv` files

merged <- read_delim("Data/merged.csv", 
    "\t", escape_double = FALSE, col_types = cols(image_id = col_number(), 
        latitude = col_number(), longitude = col_number()), 
    trim_ws = TRUE)

```

### Reviewing Data

We can run some preliminary reviews and scans of the data set to make sure it was imported correctly.

A summary command shows us basic analysis of each variable. We can see image_id, latitude, and longitude have been important as numeric. We also see there are 13 NAs which need to be removed.

```{r summary of merged data}
summary(merged)
```

Using skim() from the skimr package we can see the total number of rows and columns, as well as analysis by variable type.

```{r skim of merged data}
skimr::skim(merged)
```


### Data Cleaning

Data is cleaned as one dataset before being separated into windows of time.

The first set is to remove the 13 NAs revealed in the summary.

```{r remove NAs}

merged <- na.omit(merged) 

```

We also need to remove all suspected non-tourists from the dataset, however, the methology we use requires us to separate the dataset into windows of time before we do that.

### Creating Windows of Time for Analysis

By creating windows of time we are able to research whether a change has happened over the course of a period of time. For this first experiment we are separating the dataset into subsets of 3 years: 2007-2009, 2010-2012, 2013-2015, and 2016-2018.

First, we separate the DateTime column into just a date column.

```{r filter by time}
merged$Date <- as.Date(merged$date_taken) 

```

We use a dplyr function to separate the dataset into subsets.

```{r create paths and windows by years}
# 
data_2007to2009 <- merged %>%
                        filter(Date >= as.Date('2007-01-01') & Date <= as.Date('2009-12-31'))

data_2010to2012 <- merged %>%
                        filter(Date >= as.Date('2010-01-01') & Date <= as.Date('2012-12-31'))

data_2013to2015 <- merged %>%
                        filter(Date >= as.Date('2013-01-01') & Date <= as.Date('2015-12-31'))

data_2016to2018 <- merged %>%
                        filter(Date >= as.Date('2016-01-01') & Date <= as.Date('2018-12-31'))
```

## Separating Tourists from Non-Tourists

How do you know who is a tourist and who isn't a tourist? We won't know 100% for sure, but we can run certain data cleaning techniques to make it as likely as possible our dataset will only contain tourists.

First, we make sure each user has appeared more than once in the 2016 to 2018 dataset.

```{r 2018 filter users}

## User must appear more than once
data_2016to2018 <- data_2016to2018%>%
                      group_by(owner_id) %>%
                      filter(n()>2)%>%
                      arrange(owner_id,date_taken)
```

## Initial Analysis with Example

After spearating tourists from non-tourists, we run an initial mapping of the 2016 to 2018 time window.

### 2016 to 2018 Map of Photos

Using the leaflet library we can run a prelimary map to see where the photos were taken in the 2016 to 2018 dataset.

```{r 2018 map}
m <- leaflet() %>%
  addTiles() %>%  # Add default OpenStreetMap map tiles
  addCircleMarkers(lng= data_2016to2018$longitude, 
                   lat= data_2016to2018$latitude,
                   weight = 1, 
                   radius = 2,
                   color = "blue",
                   fillOpacity = 0.4)
m  # Print the map
```

We can see there are a lot of photos. In order to make analysis more streamlined, we'll perform a clustering methodlogy.

## Clustering using DBSCAN

Density-based spatial clustering of applications with noise (DBSCAN) is a data clustering algorithm it groups together points that are closely packed together, marking as outliers points that lie alone in low-density regions. It is a very commom clustering methodolgy and by Using this methodology we can combine photo locations that are close together.

```{r clustering 2018}

dbs_merged <- dbscan(data_2016to2018[,3:4], eps = 0.000055, minPts = 20)
data_2016to2018$cluster <- dbs_merged$cluster

summary(dbs_merged)
str(dbs_merged)
```

After clustering we can map these clusters...

```{r mapping clustering 2018}
groups  <- data_2016to2018 %>% filter(cluster != 0)
noise  <- data_2016to2018 %>% filter(cluster == 0)

m <- leaflet() %>%
  addTiles() %>%  # Add default OpenStreetMap map tiles
  addCircleMarkers(lng= groups$longitude, 
                   lat= groups$latitude,
                   weight = 1, 
                   radius = 2,
                   color = "blue",
                   fillOpacity = 0.4)
m  # Print the map
```

... and see a large difference! These are the locations that have a concentration of photos taken between 2016 and 2018.


Eventually, we will repeat this for each window of time - but first we want to try and detect tourist paths within this dataset.


## Detecting Tourist Paths



